{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkKe_BQdwLVG"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "2bbzcOxzwUq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import math\n",
        "from math import log\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "# Compute TF-IDF using scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Initialize NLTK's Porter Stemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "716d3BOUzb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline with the desired model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Define the prompts\n",
        "topics = [\n",
        "    \"machine learning\",\n",
        "    \"Lionel Messi\"\n",
        "]\n",
        "\n",
        "# List to store generated documents\n",
        "generated_documents = []\n",
        "\n",
        "# Generate text based on each topic and store the documents\n",
        "for i, topic in enumerate(topics, start=1):\n",
        "    generated_text = generator(topic, max_length=50)\n",
        "    generated_documents.append(generated_text[0]['generated_text'])\n",
        "\n",
        "# Print the generated documents\n",
        "print(\"Generated Documents:\")\n",
        "for i, doc in enumerate(generated_documents, start=1):\n",
        "    print(\"Document\", i, \":\", doc)\n",
        "    print()"
      ],
      "metadata": {
        "id": "x8Qe6oIXwfXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLTK's stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean and normalize the text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove symbols and characters\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split the text into words\n",
        "    words = cleaned_text.split()\n",
        "    # Remove stop words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Stem each word\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in generated_documents]"
      ],
      "metadata": {
        "id": "s5NnE01kwhuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Term Frequency (TF)\n",
        "def compute_tf(document):\n",
        "    tf = Counter(document)\n",
        "    for word in tf:\n",
        "        tf[word] = tf[word] / len(document)\n",
        "    return tf\n",
        "\n",
        "# Compute Inverse Document Frequency (IDF)\n",
        "def compute_idf(documents):\n",
        "    idf = {}\n",
        "    total_docs = len(documents)\n",
        "    words_in_documents = [set(document) for document in documents]\n",
        "    all_words = set().union(*words_in_documents)\n",
        "    for word in all_words:\n",
        "        doc_count = sum([1 for document in documents if word in document])\n",
        "        idf[word] = log(total_docs / (doc_count + 1))\n",
        "    return idf\n",
        "\n",
        "# Compute TF-IDF\n",
        "def compute_tfidf(documents):\n",
        "    tfidf = []\n",
        "    idf = compute_idf(documents)\n",
        "    for document in documents:\n",
        "        tf = compute_tf(document)\n",
        "        tfidf_doc = {word: tf[word] * idf[word] for word in tf}\n",
        "        tfidf.append(tfidf_doc)\n",
        "    return tfidf\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_documents = [' '.join(doc) for doc in preprocessed_documents]\n",
        "\n",
        "# Compute TF-IDF using scikit-learn\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print TF-IDF using scikit-learn\n",
        "print(\"TF-IDF using scikit-learn:\")\n",
        "for i in range(len(preprocessed_documents)):\n",
        "    print(\"Document\", i+1, \":\")\n",
        "    for j, word in enumerate(feature_names):\n",
        "        tfidf_value = tfidf_matrix[i, j]\n",
        "        if tfidf_value != 0:\n",
        "            print(word, \":\", tfidf_value)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "q0tngRn0wku7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}