{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkKe_BQdwLVG"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "2bbzcOxzwUq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import math\n",
        "from math import log\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import pipeline\n",
        "# Compute TF-IDF using scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Initialize NLTK's Porter Stemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "716d3BOUzb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the text generation pipeline with the desired model\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Define the prompts\n",
        "topics = [\n",
        "    \"machine learning\",\n",
        "    \"Lionel Messi\"\n",
        "]\n",
        "\n",
        "# List to store generated documents\n",
        "generated_documents = []\n",
        "\n",
        "# Generate text based on each topic and store the documents\n",
        "for i, topic in enumerate(topics, start=1):\n",
        "    generated_text = generator(topic, max_length=50)\n",
        "    generated_documents.append(generated_text[0]['generated_text'])\n",
        "\n",
        "# Print the generated documents\n",
        "print(\"Generated Documents:\")\n",
        "for i, doc in enumerate(generated_documents, start=1):\n",
        "    print(\"Document\", i, \":\", doc)\n",
        "    print()"
      ],
      "metadata": {
        "id": "x8Qe6oIXwfXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLTK's stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean and normalize the text\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove symbols and characters\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Split the text into words\n",
        "    words = cleaned_text.split()\n",
        "    # Remove stop words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Stem each word\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in generated_documents]"
      ],
      "metadata": {
        "id": "s5NnE01kwhuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF\n",
        "def compute_tf(document):\n",
        "    tf = Counter(document)\n",
        "    total_terms = len(document)\n",
        "    tf = {term: freq / total_terms for term, freq in tf.items()}\n",
        "    return tf\n",
        "\n",
        "# Compute IDF\n",
        "def compute_idf(documents):\n",
        "    idf = {}\n",
        "    total_docs = len(documents)\n",
        "    all_terms = set(term for document in documents for term in document)\n",
        "    for term in all_terms:\n",
        "        doc_count = sum(1 for document in documents if term in document)\n",
        "        idf[term] = math.log(1 + doc_count / total_docs)\n",
        "    return idf\n",
        "\n",
        "# Compute TF-IDF\n",
        "def compute_tfidf(document, documents):\n",
        "    tf = compute_tf(document)\n",
        "    idf = compute_idf(documents)\n",
        "    tfidf = {term: tf[term] * idf[term] for term in tf}\n",
        "    return tfidf\n",
        "\n",
        "# Preprocess the documents\n",
        "preprocessed_documents = [' '.join(doc) for doc in preprocessed_documents]\n",
        "\n",
        "# Compute TF-IDF\n",
        "tfidf_docs = []\n",
        "for doc in preprocessed_documents:\n",
        "    terms = doc.split()\n",
        "    tfidf = compute_tfidf(terms, preprocessed_documents)\n",
        "    tfidf_docs.append(tfidf)\n",
        "\n",
        "# Print TF-IDF\n",
        "print(\"TF-IDF:\")\n",
        "for i, doc in enumerate(tfidf_docs, start=1):\n",
        "    print(\"Document\", i, \":\")\n",
        "    for term, value in doc.items():\n",
        "        print(term, \":\", value)\n",
        "    print()"
      ],
      "metadata": {
        "id": "q0tngRn0wku7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}